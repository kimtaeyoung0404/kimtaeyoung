# crawling-data
데이터 크롤링
####  참고 링크 
```
https://keep-steady.tistory.com/29
```
#### 노트북에서 좌측 하단에 cmd 창을 열고 시작

``` bash
C:\Users\user>cd ..

C:\Users>cd ..

C:\>mkdir project_ai
cd mkdir project_ai
```
```
git clone --depth 1 https://github.com/YoongiKim/AutoCrawler
```
<b>
  결과
  
remote: Counting objects: 100% (12/12), done.
remote: Compressing objects: 100% (11/11), done.
remote: Total 12 (delta 0), reused 8 (delta 0), pack-reused 0
Receiving objects: 100% (12/12), 16.48 MiB | 2.30 MiB/s, done.

```
cd AutoCrawler

C:\project_ai\AutoCrawler>pip install -r requirements.txt
```
<b>
손흥민
이정후
타이거 우즈
찰리 우즈

를 내 컴퓨터 -> 로컬 디스크 C->projetcs-> AutoCroawler-> kewwords.txt 에 dog, cat 지우고 넣어준 후 저장
크롤링 실행
```
C:\project_ai\AutoCrawler>python main.py  --limit 20

```
<b>   EfficientNet-pytorch 설치
```
$pip install efficientnet_pytorch
$pip install scikit-learn
$git clone https://github.com/lukemelas/EfficientNet-PyTorch
$cd EfficientNet-Pytorch
$pip install -e .
```
![aa](https://github.com/user-attachments/assets/86482ef3-9988-44c5-9a4e-3747cc5b4a21)

![bb](https://github.com/user-attachments/assets/11714644-0dc9-43ed-b158-97658841f846)
![cc](https://github.com/user-attachments/assets/bd1f429b-8c5f-4a9a-bccb-9294219792ab)
![dd](https://github.com/user-attachments/assets/be247eba-3781-40f6-9460-353558726011)
